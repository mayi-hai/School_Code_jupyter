{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   txt  chap\n",
      "0    忽听得巨钟下的铜缸内当当当响声不绝，不知里面是何怪物，众僧面面相觑，手足无措，齐声口诵《高王...   3.0\n",
      "1    柯镇恶双腿中剑，受伤不轻，神智却仍清明，从怀中摸出解毒药来，命僧人分别去给丘处机及韩小莹服下...   3.0\n",
      "2    朱聪与南希仁所受内伤甚重。全金发腰间所受的这一脚也着实不轻。张阿生胳臂折断，胸口受震，一时痛...   3.0\n",
      "3    过了数日，丘处机与韩小莹身上所中的毒都消解了。丘处机精通医道，开了药方给朱聪等人调治，又分别...   3.0\n",
      "4    过了一会，韩小莹首先说道：“丘道长能干英明，天下皆知，我们七兄弟也不是初走江湖之人，这次大家...   3.0\n",
      "..                                                 ...   ...\n",
      "166  乃蛮兵的统帅见形势不利，带领人马往高地上抢来。蒙古兵竖起了软墙。那是数层羊毛厚毡所制，用以挡...   3.0\n",
      "167  两员勇将这么一阵冲击，乃蛮后军登时大乱，前军也不由得军心摇动。统兵的将军正自犹豫不决，札木合...   3.0\n",
      "168  铁木真下令剥下乃蛮兵的衣甲，将二千余名降兵连人带马分成四份，给完颜兄弟一份，义父王罕一份，义...   3.0\n",
      "169  完颜洪熙这时才惊魂大定，兴高采烈地不住议论刚才的战斗，笑道：“他们要讨官职，六弟，咱们封他一...   3.0\n",
      "170  完颜洪烈见铁木真和札木合以少胜多，这一仗打得光彩之极，不觉暗暗心惊，心想：“现下北方各部自相...   3.0\n",
      "\n",
      "[171 rows x 2 columns]\n",
      "                                                   txt  chap\n",
      "0    完颜洪熙笑道：“好，再打他个痛快。”蒙古兵前哨报来：“王罕亲自前来迎接大金国两位太子。”铁木...   4.0\n",
      "1    沙尘中一彪军马涌到。数百名亲兵拥卫下，王罕驰马近前，滚下马背，双手分别携着铁木真和札木合两个...   4.0\n",
      "2    王罕道：“小人听说乃蛮人要待无礼，只怕惊动了两位王子，急忙带兵赶来，幸喜仗着两位殿下的威风，...   4.0\n",
      "3    王罕所得的封号，又比铁木真为高，反正只是虚衔，金国也不吝惜。王罕高兴之极，对完颜兄弟连声道谢...   4.0\n",
      "4    酒到半酣，完颜洪烈道：“老英雄威名远震，我们在中都也久已听闻，那是不消说了。蒙古人年轻一辈中...   4.0\n",
      "..                                                 ...   ...\n",
      "162  朱聪眼中含了泪水，向郭靖道：“你到这里，是想来跟我们学本事？”郭靖道：“是。”朱聪道：“那么...   4.0\n",
      "163  张阿生惨然一笑，道：“够啦！”强忍疼痛，说道：“好孩子，我没能教你本事……唉，其实你学会了我...   4.0\n",
      "164  韩小莹把耳朵凑到他嘴边，只听他说道：“教好孩子，别输给了……臭道士……”韩小莹道：“你放心，...   4.0\n",
      "165  六怪伏地大哭。他七人义结金兰，本已情如骨肉，这些年来为了追寻郭靖母子而远来大漠，更无一日分离...   4.0\n",
      "166  全金发和韩宝驹下山查看梅超风的踪迹。狂风大雨之后，沙漠上的足迹已全然不见，不知她逃向何处。两...   4.0\n",
      "\n",
      "[167 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'raw12ana' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-953b4c556cee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mcuttxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 这里不做任何清理工作，以保留情感词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mraw12ana\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cleantxt\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw12ana\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuttxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mraw12ana\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# 加载数据集合及其对应的分类\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw12ana' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import *\n",
    "from functools import reduce\n",
    "import jieba\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"utf-8\" ,engine='python')\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw['head'] = raw.txt.apply(m_head)\n",
    "raw['mid'] = raw.txt.apply(m_mid)\n",
    "raw['len'] = raw.txt.apply(len)\n",
    "\n",
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw)):\n",
    "    if raw['head'][i] == \"第\" and raw['mid'][i] > 0 and raw['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "del raw['len']\n",
    "\n",
    "# 从原始语料df中提取出所需的前三四章段落\n",
    "raw13 = raw[raw.chap.isin([3])]\n",
    "raw13ana = raw13.iloc[list(raw13.txt.apply(len) > 50), :] # 只使用超过50字的段落\n",
    "raw13ana.reset_index(drop = True, inplace = True)\n",
    "\n",
    "raw14 = raw[raw.chap.isin([4])]\n",
    "raw14ana = raw14.iloc[list(raw14.txt.apply(len) > 50), :] # 只使用超过50字的段落\n",
    "raw14ana.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# 分词和预处理\n",
    "cuttxt = lambda x: \" \".join(jieba.lcut(x)) # 这里不做任何清理工作，以保留情感词\n",
    "raw12ana[\"cleantxt\"] = raw12ana.txt.apply(cuttxt) \n",
    "raw12ana.head()\n",
    "# 加载数据集合及其对应的分类\n",
    "def h_split(h_list,ratio):\n",
    "    n_total = len(h_list)\n",
    "    print(n_total)\n",
    "    offset = int(n_total * ratio)\n",
    "    sublist_1 = h_list[:offset]\n",
    "    sublist_2 = h_list[offset:]\n",
    "    return sublist_1,sublist_2\n",
    "#--0代表第三章\n",
    "#--1代表第四章\n",
    "import jieba.analyse\n",
    "\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "X1,Y1=h_split(raw13ana[\"txt\"],0.8)\n",
    "X2,Y2=h_split(raw14ana[\"txt\"],0.8)\n",
    "\n",
    "for i in X1:\n",
    "    h=jieba.analyse.extract_tags(i)\n",
    "    X_train.append(h)\n",
    "    Y_train.append(0)\n",
    "for i in X2:\n",
    "    h=jieba.analyse.extract_tags(i)\n",
    "    X_train.append(h)\n",
    "    Y_train.append(1)\n",
    "    \n",
    "for j in Y1:\n",
    "    h=jieba.analyse.extract_tags(j)\n",
    "    X_test.append(h)\n",
    "    Y_test.append(0)\n",
    "for j in Y2:\n",
    "    h=jieba.analyse.extract_tags(j)\n",
    "    X_test.append(h)\n",
    "    Y_test.append(1)\n",
    "    \n",
    "print(X_train)\n",
    "print(Y_train)\n",
    "\n",
    "# 标识\n",
    "adClass = 1\n",
    "def loadDataSet():\n",
    "    wordsList = X_train\n",
    "    # 1 是, 0 否\n",
    "    classVec = Y_train\n",
    "    return wordsList, classVec\n",
    "\n",
    "def doc2VecList(docList):\n",
    "    a = list(reduce(lambda x, y: set(x) | set(y), docList))\n",
    "    return a\n",
    "\n",
    "def words2Vec(vecList, inputWords):\n",
    "    resultVec = [0] * len(vecList)\n",
    "    for word in inputWords:\n",
    "        if word in vecList:\n",
    "            resultVec[vecList.index(word)] += 1\n",
    "        else:\n",
    "            continue\n",
    "    return array(resultVec)\n",
    "\n",
    "def trainNB(trainMatrix, trainClass):\n",
    "    numTrainClass = len(trainClass)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    p0Words = 2.0\n",
    "    p1Words = 2.0\n",
    "    for i in range(numTrainClass):\n",
    "        if trainClass[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Words += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Words += sum(trainMatrix[i])\n",
    "    p0Vec = log(p0Num / p0Words)\n",
    "    p1Vec = log(p1Num / p1Words)\n",
    "    pClass1 = sum(trainClass) / float(numTrainClass)\n",
    "    return p0Vec, p1Vec, pClass1\n",
    "\n",
    "def classifyNB(testVec, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(testVec * p1Vec) + log(pClass1)\n",
    "    p0 = sum(testVec * p0Vec) + log(1 - pClass1)\n",
    "    if p0 > p1:\n",
    "        return 0\n",
    "    return 1\n",
    "def printClass(words, testClass):\n",
    "    if testClass == adClass:\n",
    "        print(words, '第四章')\n",
    "    else:\n",
    "        print(words, '第三章')\n",
    "\n",
    "# 从训练数据集中提取出属性矩阵和分类数据\n",
    "docList, classVec = loadDataSet()\n",
    "allWordsVec = doc2VecList(docList)\n",
    "trainMat = list(map(lambda x: words2Vec(allWordsVec, x), docList))\n",
    "p0V, p1V, pClass1 = trainNB(trainMat, classVec)\n",
    "predict=[]  \n",
    "for k in X_test:\n",
    "    testWords = k\n",
    "    testVec = words2Vec(allWordsVec, testWords)\n",
    "    testClass = classifyNB(testVec, p0V, p1V, pClass1)\n",
    "    predict.append(testClass)\n",
    "    printClass(testWords, testClass)\n",
    "# 分类报告：precision/recall/fi-score/均值/分类个数\n",
    "from sklearn.metrics import classification_report\n",
    "class_true = Y_test #正确的分类结果\n",
    "class_pred = predict #实际的分类结果\n",
    "target_names = ['第三章', '第四章']\n",
    "print(classification_report(class_true, class_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10968\n",
      "                                                     txt head  mid  len\n",
      "0                                                书名：天龙八部    书   -1    7\n",
      "1                                                  作者：金庸    作   -1    5\n",
      "2      ----------------------------------------------...    -   -1   61\n",
      "3                                ☆本文由早安电子书网友分享，版权归原作者所有☆    ☆   -1   23\n",
      "4                                      ☆请勿用于商业行为，一切后果自负☆    ☆   -1   17\n",
      "...                                                  ...  ...  ...  ...\n",
      "10963                            ☆本文由早安电子书网友分享，版权归原作者所有☆    ☆   -1   23\n",
      "10964                                  ☆请勿用于商业行为，一切后果自负☆    ☆   -1   17\n",
      "10965                                            ☆早安电子书☆    ☆   -1    7\n",
      "10966                             ☆http://www.zadzs.com☆    ☆   -1   22\n",
      "10967  ----------------------------------------------...    -   -1   61\n",
      "\n",
      "[10968 rows x 4 columns]\n",
      "                                                   txt  chap\n",
      "0    完颜洪熙笑道：“好，再打他个痛快。”蒙古兵前哨报来：“王罕亲自前来迎接大金国两位太子。”铁木...   4.0\n",
      "1    沙尘中一彪军马涌到。数百名亲兵拥卫下，王罕驰马近前，滚下马背，双手分别携着铁木真和札木合两个...   4.0\n",
      "2    王罕道：“小人听说乃蛮人要待无礼，只怕惊动了两位王子，急忙带兵赶来，幸喜仗着两位殿下的威风，...   4.0\n",
      "3    王罕所得的封号，又比铁木真为高，反正只是虚衔，金国也不吝惜。王罕高兴之极，对完颜兄弟连声道谢...   4.0\n",
      "4    酒到半酣，完颜洪烈道：“老英雄威名远震，我们在中都也久已听闻，那是不消说了。蒙古人年轻一辈中...   4.0\n",
      "..                                                 ...   ...\n",
      "162  朱聪眼中含了泪水，向郭靖道：“你到这里，是想来跟我们学本事？”郭靖道：“是。”朱聪道：“那么...   4.0\n",
      "163  张阿生惨然一笑，道：“够啦！”强忍疼痛，说道：“好孩子，我没能教你本事……唉，其实你学会了我...   4.0\n",
      "164  韩小莹把耳朵凑到他嘴边，只听他说道：“教好孩子，别输给了……臭道士……”韩小莹道：“你放心，...   4.0\n",
      "165  六怪伏地大哭。他七人义结金兰，本已情如骨肉，这些年来为了追寻郭靖母子而远来大漠，更无一日分离...   4.0\n",
      "166  全金发和韩宝驹下山查看梅超风的踪迹。狂风大雨之后，沙漠上的足迹已全然不见，不知她逃向何处。两...   4.0\n",
      "\n",
      "[167 rows x 2 columns]\n",
      "                                                   txt  chap\n",
      "0    奔出数里，黑玫瑰走上了一条长岭，山岭渐见崎岖，黑玫瑰行得更加慢了，背后呐喊声隐隐传来。段誉叫...   4.0\n",
      "1    黑玫瑰奋蹄加快脚步，突然之间，前面出现一条深涧，阔约数丈，黑黝黝的深不见底。黑玫瑰一声惊嘶，...   4.0\n",
      "2    木婉清见前无去路，后有追兵，问道：“我要纵马跳将过去。你随我冒险呢，还是留下来？”段誉心想：...   4.0\n",
      "3    黑玫瑰放开四蹄，急奔而前，到得深涧边上，使劲纵跃，直窜了过去。段誉但觉腾云驾雾一般，一颗心也...   4.0\n",
      "4    黑玫瑰受了主人催逼，出尽全力的这么一跃，前脚双蹄勉强踏到了对岸，但两边实是相距太宽，它彻夜奔...   4.0\n",
      "..                                                 ...   ...\n",
      "207  如此心神不定，一晃又是数日。度日如年的滋味，这几天中当真尝得透了。日日夜夜，只盼山峰下传上来...   4.0\n",
      "208  越等越苦，师父所说“天下男子无不负心薄幸”之言尽在耳边响个不住，自己虽说“段郎未必如此”，终...   4.0\n",
      "209  那三人等候“恶贯满盈”这天下第一恶人到来，心情之焦急虽然及不上她，可也是有如热锅上蚂蚁一般，...   4.0\n",
      "210  到得第六天晚间，木婉清心想：“明日是最后一天，这负心郎是决计不来的了。今晚乘着天黑，须得悄悄...   4.0\n",
      "211  几次三番拔足欲行，总是牵挂着段誉：“倘若这负心郎明天来找我呢？明天如不能和他相见，此后便永无...   4.0\n",
      "\n",
      "[212 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'raw12ana' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1b7138be9795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[0mcuttxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 这里不做任何清理工作，以保留情感词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0mraw12ana\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cleantxt\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw12ana\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuttxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[0mraw12ana\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mh_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw12ana' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import *\n",
    "from functools import reduce\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "raw1 = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"utf-8\" ,engine='python')\n",
    "raw2 = pd.read_csv(\"附件3_金庸-天龙八部txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"utf-8\" ,engine='python')\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw1['head'] = raw1.txt.apply(m_head)\n",
    "raw1['mid'] = raw1.txt.apply(m_mid)\n",
    "raw1['len'] = raw1.txt.apply(len)\n",
    "# raw['chap'] = 0\n",
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw1)):\n",
    "    if raw1['head'][i] == \"第\" and raw1['mid'][i] > 0 and raw1['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw1['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw1.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw1['head']\n",
    "del raw1['mid']\n",
    "del raw1['len']\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\" \")\n",
    "raw2['head'] = raw2.txt.apply(m_head)\n",
    "raw2['mid'] = raw2.txt.apply(m_mid)\n",
    "raw2['len'] = raw2.txt.apply(len)\n",
    "# raw['chap'] = 0\n",
    "print(raw2)\n",
    "# 章节判断\n",
    "\n",
    "chapnum = 0\n",
    "for i in range(len(raw2)):\n",
    "    r1 = re.compile(u'[一二三四五六七八九十]{1,} ')    #匹配各章节\n",
    "    if r1 and raw2['mid'][i] > 0 and raw2['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw2['txt'][i] == \"附录：陈世骧先生书函\" :\n",
    "        chapnum = 0\n",
    "    raw2.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw2['head']\n",
    "del raw2['mid']\n",
    "del raw2['len']\n",
    "raw2.head(50)\n",
    "# 从原始语料提取出所需的射雕与天龙的第四章\n",
    "raw13 = raw1[raw1.chap.isin([4])]\n",
    "raw13ana = raw13.iloc[list(raw13.txt.apply(len) > 50), :] # 只使用超过50字的段落\n",
    "raw13ana.reset_index(drop = True, inplace = True)\n",
    "print(raw13ana)\n",
    "\n",
    "raw14 = raw2[raw2.chap.isin([4])]\n",
    "raw14ana = raw14.iloc[list(raw14.txt.apply(len) > 50), :] # 只使用超过50字的段落\n",
    "raw14ana.reset_index(drop = True, inplace = True)\n",
    "print(raw14ana)\n",
    "# 分词和预处理\n",
    "\n",
    "\n",
    "cuttxt = lambda x: \" \".join(jieba.lcut(x)) # 这里不做任何清理工作，以保留情感词\n",
    "raw12ana[\"cleantxt\"] = raw12ana.txt.apply(cuttxt) \n",
    "raw12ana.head()\n",
    "def h_split(h_list,ratio):\n",
    "    n_total = len(h_list)\n",
    "    print(n_total)\n",
    "    offset = int(n_total * ratio)\n",
    "    sublist_1 = h_list[:offset]\n",
    "    sublist_2 = h_list[offset:]\n",
    "    return sublist_1,sublist_2\n",
    "#--0代表射雕\n",
    "#--1代表天龙\n",
    "import jieba.analyse\n",
    "\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "X1,Y1=h_split(raw13ana[\"txt\"],0.8)\n",
    "X2,Y2=h_split(raw14ana[\"txt\"],0.8)\n",
    "\n",
    "for i in X1:\n",
    "    h=jieba.analyse.extract_tags(i)\n",
    "    X_train.append(h)\n",
    "    Y_train.append(0)\n",
    "for i in X2:\n",
    "    h=jieba.analyse.extract_tags(i)\n",
    "    X_train.append(h)\n",
    "    Y_train.append(1)\n",
    "    \n",
    "for j in Y1:\n",
    "    h=jieba.analyse.extract_tags(j)\n",
    "    X_test.append(h)\n",
    "    Y_test.append(0)\n",
    "for j in Y2:\n",
    "    h=jieba.analyse.extract_tags(j)\n",
    "    X_test.append(h)\n",
    "    Y_test.append(1)\n",
    "\n",
    "# 标识\n",
    "adClass = 1\n",
    "\n",
    "def loadDataSet():   \n",
    "    wordsList = X_train\n",
    "    # 1 是, 0 否\n",
    "    classVec = Y_train\n",
    "    return wordsList, classVec\n",
    "\n",
    "def doc2VecList(docList):\n",
    "    a = list(reduce(lambda x, y: set(x) | set(y), docList))\n",
    "    return a\n",
    "\n",
    "def words2Vec(vecList, inputWords):\n",
    "    resultVec = [0] * len(vecList)\n",
    "    for word in inputWords:\n",
    "        if word in vecList:\n",
    "            resultVec[vecList.index(word)] += 1\n",
    "        else:\n",
    "            continue\n",
    "    return array(resultVec)\n",
    "def trainNB(trainMatrix, trainClass):\n",
    "    numTrainClass = len(trainClass)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    p0Words = 2.0\n",
    "    p1Words = 2.0\n",
    "    for i in range(numTrainClass):\n",
    "        if trainClass[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Words += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Words += sum(trainMatrix[i])\n",
    "    p0Vec = log(p0Num / p0Words)\n",
    "    p1Vec = log(p1Num / p1Words)\n",
    "    pClass1 = sum(trainClass) / float(numTrainClass)\n",
    "    return p0Vec, p1Vec, pClass1\n",
    "def classifyNB(testVec, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(testVec * p1Vec) + log(pClass1)\n",
    "    p0 = sum(testVec * p0Vec) + log(1 - pClass1)\n",
    "    if p0 > p1:\n",
    "        return 0\n",
    "    return 1\n",
    "def printClass(words, testClass):\n",
    "    if testClass == adClass:\n",
    "        print(words, '天龙')\n",
    "    else:\n",
    "        print(words, '射雕')\n",
    "\n",
    "# 从训练数据集中提取出属性矩阵和分类数据\n",
    "docList, classVec = loadDataSet()\n",
    "allWordsVec = doc2VecList(docList)\n",
    "trainMat = list(map(lambda x: words2Vec(allWordsVec, x), docList))\n",
    "p0V, p1V, pClass1 = trainNB(trainMat, classVec)\n",
    "predict=[]   #记录预测结果\n",
    "# 测试数据集\n",
    "for k in X_test:\n",
    "    testWords = k\n",
    "    testVec = words2Vec(allWordsVec, testWords)\n",
    "    testClass = classifyNB(testVec, p0V, p1V, pClass1)\n",
    "    predict.append(testClass)\n",
    "    # 打印出测试结果\n",
    "    printClass(testWords, testClass)\n",
    "# 分类报告：precision/recall/fi-score/均值/分类个数\n",
    "from sklearn.metrics import classification_report\n",
    "class_true = Y_test #正确的分类结果\n",
    "class_pred = predict #实际的分类结果\n",
    "target_names = ['射雕', '天龙']\n",
    "print(classification_report(class_true, class_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
