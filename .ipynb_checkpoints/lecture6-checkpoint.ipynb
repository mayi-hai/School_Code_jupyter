{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抽取文档主题\n",
    "\n",
    "## 什么是主题模型\n",
    "\n",
    "### LDA，Latent Dirichlet Allocation\n",
    "\n",
    "* Q: 有这么一篇文章，里面提到了詹姆斯、湖人队、季后赛，请问这篇文章最可能的主题是什么？\n",
    "\n",
    "    * 军事\n",
    "    * 体育\n",
    "    * 养生\n",
    "    * 教育\n",
    "    \n",
    "### LDA由Blei于2003年提出，其基本思想是把文档看成各种隐含主题的混合，而每个主题则表现为跟该主题相关的词项的概率分布\n",
    "\n",
    "* 该方法不需要任何关于文本的背景知识\n",
    "* 隐含主题的引入使得分析者可以对“一词多义”和“一义多词”的语言现象进行建模，更接近人类语言交互的特征\n",
    "\n",
    "### LDA基于词袋模型构建，认为文档和单词都是可交换的，忽略单词在文档中的顺序和文档在语料库中的顺序，从而将文本信息转化为易于建模的数字信息\n",
    "\n",
    "* 主题就是一个桶，里面装了出现概率较高的单词，这些单词与这个主题有很强的的相关性\n",
    "\n",
    "## LDA模型包含词项、主题和文档三层结构\n",
    "\n",
    "### 本质上，LDA简单粗暴的认为：文章中的每个词都是通过“以一定概率选择某个主题，再从该主题中以一定概率选择某个词”得到的\n",
    "\n",
    "### 一个词可能会关联很多主题，因此需要计算各种情况下的概率分布，来确定最可能出现的主题是哪种\n",
    "\n",
    "* 体育：{姚明：0.3，篮球：0.5，拳击：0.2，李现：0.03，王宝强：0.03，杨紫：0.04}\n",
    "* 娱乐：{姚明：0.03，篮球：0.03，足球：0.04，李现：0.6，王宝强：0.7，杨紫：0.8}\n",
    "\n",
    "### 一篇文章可能会涉及到几个主题，因此也需要计算多个主题的概率\n",
    "\n",
    "* 体育新闻：\\[废话，体育，体育，体育，....，娱乐，娱乐\\]\n",
    "* 八卦消息：\\[废话，废话，废话，废话，....，娱乐，娱乐\\]\n",
    "\n",
    "## LDA中涉及到的数学知识\n",
    "\n",
    "### 多项式分布：主题和词汇的概率分布服从多项式分布\n",
    "\n",
    "* 如果1个词汇主题，就是大家熟悉的二项分布\n",
    "\n",
    "### Dirichlet分布：上述多项式分布的参数为随机变量，均服从Dirichlet分布\n",
    "\n",
    "### Gibbs抽样：直接求LDA的精确参数分布计算量太大，实际上不可行，因此通过Gibbs抽烟减小计算量，得到逼近的结果\n",
    "\n",
    "* 通过现有文章（已有主题，或者需要提取主题）训练处LDA模型\n",
    "* 用模型预测新的文章所属主题分类\n",
    "\n",
    "### 主题模型对于  ***短文本***  效果不好\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主题模型的sklearn实现\n",
    "\n",
    "在scikit-learn中，LDA主题模型的类被放置在sklearn.decomposition.LatentDirichletAllocation类中，其算法实现主要基于变分推断EM算法，而没有使用基于Gibbs采样的MCMC算法实现。\n",
    "\n",
    "注意由于LDA是基于词频统计的，因此理论上一般不宜用TF-IDF来做文档特征，但并非不能尝试。实际分析中也确实会见到此类操作。\n",
    "\n",
    "> class sklearn.decomposition.LatentDirichletAllocation(\n",
    "> \n",
    "> n_components = None : 隐含主题数K，需要设置的最重要参数。\n",
    "    K的设定范围和具体的研究背景有关。\n",
    "    K越大，需要的文档样本越多。\n",
    "> \n",
    "> doc_topic_prior = None : 文档主题先验Dirichlet分布的参数α，未设定则用1/K。\n",
    "> \n",
    "> topic_word_prior = None : 主题词先验Dirichlet分布的参数η，未设定则用1/K。\n",
    "> \n",
    "> learning_method = 'online' : 即LDA的求解算法。'batch' | 'online'\n",
    "    batch: 变分推断EM算法，会将将训练样本分批用于更新主题词分布，新版默认算法。\n",
    "        样本量不大只是用来学习的话用batch比较好，这样可以少很多参数要调。\n",
    "        需注意n_components(K), doc_topic_prior(α), topic_word_prior(η)\n",
    "    online: 在线变分推断EM算法，大样本时首选。\n",
    "        需进一步注意learning_decay, learning_offset，\n",
    "            total_samples和batch_size等参数。\n",
    "> \n",
    "> 仅在online算法时需要设定的参数\n",
    ">\n",
    "> learning_decay = 0.7 ：控制\"online\"算法的学习率，一般不用修改。\n",
    "        取值最好在(0.5, 1.0]，以保证\"online\"算法渐进的收敛。\n",
    ">\n",
    "> learning_offset = 10. ：用来减小前面训练样本批次对最终模型的影响。\n",
    "        取值要大于1。\n",
    ">\n",
    ">total_samples = 1e6 ： 分步训练时每一批文档样本的数量。\n",
    "         使用partial_fit进行模型拟合时才需要此参数。\n",
    ">\n",
    "> batch_size = 128 : 每次EM算法迭代时使用的文档样本的数量。\n",
    "> \n",
    "> )\n",
    "\n",
    "***将语料库转换为所需矩阵***\n",
    "\n",
    "除直接使用分词清理后文本进行转换外，也可以先计算关键词的TF-IDF值，然后使用关键词矩阵进行后续分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 有的环境配置下read_table出错，因此改用read_csv\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"GBK\" ,engine='python')\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw['head'] = raw.txt.apply(m_head)\n",
    "raw['mid'] = raw.txt.apply(m_mid)\n",
    "raw['len'] = raw.txt.apply(len)\n",
    "# raw['chap'] = 0\n",
    "raw.head(0)\n",
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw)):\n",
    "    if raw['head'][i] == \"第\" and raw['mid'][i] > 0 and raw['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "del raw['len']\n",
    "raw.head(0)\n",
    "#提取章节\n",
    "rawgrp = raw.groupby('chap')\n",
    "chapter = rawgrp.sum()##.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\n",
    "chapter = chapter[chapter.index != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定分词及清理停用词函数\n",
    "# 熟悉Python的可以使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "stoplist = list(pd.read_csv('停用词.txt', names = ['w'], sep = 'aaa', \n",
    "                            encoding = 'utf-8', engine='python').w)\n",
    "import jieba \n",
    "def m_cut(intxt):\n",
    "    return [ w for w in jieba.cut(intxt) \n",
    "            if w not in stoplist and len(w) > 1 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.998 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 生成分词清理后章节文本\n",
    "cleanchap = [ \" \".join(m_cut(w)) for w in chapter.txt] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40x6535 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 72523 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将文本中的词语转换为词频矩阵  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer(min_df = 5) \n",
    "\n",
    "wordmtx = countvec.fit_transform(cleanchap) \n",
    "wordmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于词频矩阵X计算TF-IDF值  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(wordmtx)  \n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定LDA模型\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 10\n",
    "ldamodel = LatentDirichletAllocation(n_components = n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合LDA模型\n",
    "ldamodel.fit(wordmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合后模型的实质\n",
    "print(ldamodel.components_.shape)\n",
    "ldamodel.components_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题词打印函数\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] \n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 12\n",
    "tf_feature_names = countvec.get_feature_names()\n",
    "print_top_words(ldamodel, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim实现\n",
    "\n",
    "> class gensim.models.ldamodel.LdaModel(\n",
    ">\n",
    ">corpus = None : 用于训练模型的语料\n",
    ">\n",
    ">num_topics = 100 : 准备提取的主题数量\n",
    ">\n",
    ">id2word = None : 所使用的词条字典，便于结果阅读\n",
    ">\n",
    ">passes = 1 ：模型遍历语料库的次数，次数越多模型越精确，但是也更花时间\n",
    ">\n",
    ">)\n",
    "\n",
    "用新出现的语料更新模型\n",
    "\n",
    "    ldamodel.update(other_corpus)\n",
    "\n",
    "gensim也提供了sklearn的API接口：sklearn_api.ldamodel，可以在sklearn中直接使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定分词及清理停用词函数\n",
    "# 熟悉Python的可以使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "stoplist = list(pd.read_csv('停用词.txt', names = ['w'], sep = 'aaa', \n",
    "                            encoding = 'utf-8', engine='python').w)\n",
    "import jieba \n",
    "def m_cut(intxt):\n",
    "    return [ w for w in jieba.cut(intxt) \n",
    "            if w not in stoplist and len(w) > 1 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档预处理，提取主题词  \n",
    "chaplist = [m_cut(w) for w in chapter.txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文档对应的字典和bow稀疏向量\n",
    "from gensim import corpora, models  \n",
    "\n",
    "dictionary = corpora.Dictionary(chaplist)  \n",
    "corpus = [dictionary.doc2bow(text) for text in chaplist] # 仍为list in list  \n",
    "\n",
    "tfidf_model = models.TfidfModel(corpus) # 建立TF-IDF模型  \n",
    "corpus_tfidf = tfidf_model[corpus] # 对所需文档计算TF-IDF结果\n",
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# 列出所消耗的时间备查\n",
    "%time ldamodel = LdaModel(corpus, id2word = dictionary, \\\n",
    "                          num_topics = 10, passes = 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***列出最重要的前若干个主题***\n",
    "\n",
    "print_topics(num_topics=20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各语料的LDA模型值\n",
    "corpus_lda = ldamodel[corpus_tfidf] # 此处应当使用和模型训练时相同类型的矩阵\n",
    "\n",
    "for doc in corpus_lda:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索和文本内容最接近的主题\n",
    "query = chapter.txt[1] # 检索和第1章最接近的主题\n",
    "query_bow = dictionary.doc2bow(m_cut(query)) # 频数向量\n",
    "query_tfidf = tfidf_model[query_bow] # TF-IDF向量\n",
    "print(\"转换后：\", query_tfidf[:10])\n",
    "\n",
    "ldamodel.get_document_topics(query_bow) # 需要输入和文档对应的bow向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索和文本内容最接近的主题\n",
    "ldamodel[query_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果的图形化呈现\n",
    "\n",
    "pyLDAvis包引入自R，可以用交互式图形的方式呈现主题模型的分析结果。\n",
    "\n",
    "同时支持sklearn和gensim包。\n",
    "\n",
    "在许多系统配置下都会出现兼容问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对sklearn的LDA结果作呈现\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.sklearn.prepare(ldamodel, tfidf, countvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.disable_notebook() # 关闭notebook支持后，可以看到背后所生成的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战练习\n",
    "\n",
    "在其余参数全部固定不变的情况下，尝试分别用清理前矩阵、清理后原始矩阵、TF-IDF矩阵进行LDA模型拟合，比较分析结果。\n",
    "\n",
    "在gensim拟合LDA时，分别将passes参数设置为1、5、10、50、100等，观察结果变化的情况，思考如何对该参数做最优设定。\n",
    "\n",
    "请尝试对模型进行优化，得到对本案例较好的分析结果。\n",
    "\n",
    "提示：使用gensim进行拟合更容易一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
