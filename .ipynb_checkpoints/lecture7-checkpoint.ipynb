{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档相似度\n",
    "\n",
    "## 用途\n",
    "   * 搜索引擎的类似文章推荐   \n",
    "   * 购物网站的类似商品推荐\n",
    "   * 点评网站/微博微信平台上的类似内容推荐\n",
    "\n",
    "## 基于词袋模型的基本思路\n",
    "   * 如果两个文档/两句话的用词越相似，他们的内容就应该越相似。因此，可以从词频入手，计算他们的相似程度\n",
    "   * 文档向量化之后，相似度的考察就可以直接转化为计算空间中的距离问题\n",
    "   * 缺陷： 不能考虑否定词的巨大作用，不能考虑词序的差异\n",
    "   \n",
    "### 在本质上，向量空间中文本相似度的计算和任何聚类方法所考虑的问题***没有区别***\n",
    "\n",
    "## 余弦相似度\n",
    "### 两个向量间的夹角能够很好的反映其相似度\n",
    "\n",
    "   * 但夹角大小使用不便，因此用夹角的余弦值作为相似度衡量指标\n",
    "   * 思考：为什么只考虑夹角，不考虑相对距离?\n",
    "   \n",
    "### 余弦值越接近1，夹角越接近0度，两个向量也就越相似\n",
    "### 可以证明余弦值的计算公式可以直接扩展到n维空间\n",
    "### 因此在由n维向量所构成的空间中，可以利用余弦值来计算文档的相似度\n",
    "\n",
    "## 相似度计算：基本分析思路\n",
    "### 语料分词、清理\n",
    "  * 原始语料分词\n",
    "  * 语料清理\n",
    "### 语料向量化\n",
    "  * 将语料转换为词频向量\n",
    "  * 为了避免文章长度的差异，长度悬殊时可以考虑使用相对词频\n",
    "### 计算相似度\n",
    "  * 计算两个向量的余弦相似度，值越大表示越相似\n",
    "### 仍然存在的问题\n",
    "  * 高频词不一定具有文档代表性，导致相似度计算结果变差\n",
    "\n",
    "## 相似度计算：基本分析思路\n",
    "### 语料分词、清理\n",
    "  * 原始语料分词\n",
    "  * 语料清理\n",
    "### 语料向量化\n",
    "  * 将语料转换为基于关键词的词频向量\n",
    "  * 为了避免文章长度的差异，长度悬殊时可以考虑使用相对词频\n",
    "### 使用TF-IDF算法，找出两篇文章的关键词\n",
    "  * 例如取前20个，或者前50个\n",
    "### 计算相似度\n",
    "  * 计算两个向量的余弦相似度，值越大表示越相似\n",
    "\n",
    "### 当向量表示概率分布式，其他相似度测量方法比余弦相似度更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词条相似度：word2vec\n",
    "\n",
    "词袋模型不考虑词条之间的相关性，因此无法用于计算词条相似度。\n",
    "\n",
    "分布式表达会考虑词条的上下文关联，因此能够提取出词条上下文中的相关性信息，而词条之间的相似度就可以直接利用此类信息加以计算。\n",
    "\n",
    "目前主要使用gensim实现相应的算法。\n",
    "\n",
    "gensim也提供了sklearn的API接口：sklearn_api.w2vmodel，可以在sklearn中直接使用。\n",
    "\n",
    "设置word2vec模型\n",
    "> class gensim.models.word2vec.Word2Vec(\n",
    ">\n",
    "> sentences = None : 类似list of list的格式，对于特别大的文本，尽量考虑流式处理\n",
    ">\n",
    "> size = 100 : 词条向量的维度，数据量充足时，300/500的效果会更好\n",
    ">\n",
    "> window = 5 : 上下文窗口大小\n",
    ">\n",
    "> workers = 3 : 同时运行的线程数，多核系统可明显加速计算\n",
    ">\n",
    ">其余细节参数设定：\n",
    ">\n",
    ">    min_count = 5 : 低频词过滤阈值，低于该词频的不纳入模型\n",
    ">\n",
    ">    max_vocab_size = None : 每1千万词条需要1G内存，必要时设定该参数以节约内存\n",
    ">\n",
    ">    sample=0.001 : 负例采样的比例设定\n",
    ">\n",
    ">    negative=5 : 一般为5-20，设为0时不进行负例采样\n",
    ">\n",
    ">    iter = 5 : 模型在语料库上的迭代次数，该参数将被取消\n",
    ">\n",
    ">与神经网络模型有关的参数设定：\n",
    ">\n",
    ">    seed=1, alpha=0.025, min_alpha=0.0001, sg=0, hs=0\n",
    ">\n",
    ">)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 有的环境配置下read_table出错，因此改用read_csv\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"GBK\" ,engine='python')\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw['head'] = raw.txt.apply(m_head)\n",
    "raw['mid'] = raw.txt.apply(m_mid)\n",
    "raw['len'] = raw.txt.apply(len)\n",
    "# raw['chap'] = 0\n",
    "raw.head(0)\n",
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw)):\n",
    "    if raw['head'][i] == \"第\" and raw['mid'][i] > 0 and raw['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "del raw['len']\n",
    "raw.head(0)\n",
    "#提取章节\n",
    "rawgrp = raw.groupby('chap')\n",
    "chapter = rawgrp.sum()##.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\n",
    "chapter = chapter[chapter.index != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>第一回 风雪惊变钱塘江浩浩江水，日日夜夜无穷无休的从两浙西路临安府牛家村边绕过，东流入海。江...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>第二回 江南七怪颜烈跨出房门，过道中一个中年士人拖着鞋皮，踢跶踢跶的直响，一路打着哈欠迎面过...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>第三回 黄沙莽莽寺里僧众见焦木圆寂，尽皆悲哭。有的便为伤者包扎伤处，抬入客舍。忽听得巨钟下的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4.0</td>\n",
       "      <td>第四回 黑风双煞完颜洪熙笑道：“好，再打他个痛快。”蒙古兵前哨报来：“王罕亲自前来迎接大金国...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5.0</td>\n",
       "      <td>第五回 弯弓射雕一行人下得山来，走不多时，忽听前面猛兽大吼声一阵阵传来。韩宝驹一提缰，胯下黄...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    txt\n",
       "chap                                                   \n",
       "1.0   第一回 风雪惊变钱塘江浩浩江水，日日夜夜无穷无休的从两浙西路临安府牛家村边绕过，东流入海。江...\n",
       "2.0   第二回 江南七怪颜烈跨出房门，过道中一个中年士人拖着鞋皮，踢跶踢跶的直响，一路打着哈欠迎面过...\n",
       "3.0   第三回 黄沙莽莽寺里僧众见焦木圆寂，尽皆悲哭。有的便为伤者包扎伤处，抬入客舍。忽听得巨钟下的...\n",
       "4.0   第四回 黑风双煞完颜洪熙笑道：“好，再打他个痛快。”蒙古兵前哨报来：“王罕亲自前来迎接大金国...\n",
       "5.0   第五回 弯弓射雕一行人下得山来，走不多时，忽听前面猛兽大吼声一阵阵传来。韩宝驹一提缰，胯下黄..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.821 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>cut</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>第一回 风雪惊变钱塘江浩浩江水，日日夜夜无穷无休的从两浙西路临安府牛家村边绕过，东流入海。江...</td>\n",
       "      <td>[第一回,  , 风雪, 惊变, 钱塘江, 浩浩, 江水, ，, 日日夜夜, 无穷, 无休,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>第二回 江南七怪颜烈跨出房门，过道中一个中年士人拖着鞋皮，踢跶踢跶的直响，一路打着哈欠迎面过...</td>\n",
       "      <td>[第二回,  , 江南七怪, 颜烈, 跨出, 房门, ，, 过道, 中, 一个, 中年, 士...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>第三回 黄沙莽莽寺里僧众见焦木圆寂，尽皆悲哭。有的便为伤者包扎伤处，抬入客舍。忽听得巨钟下的...</td>\n",
       "      <td>[第三回,  , 黄沙, 莽莽, 寺里, 僧众, 见, 焦木, 圆寂, ，, 尽, 皆, 悲...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4.0</td>\n",
       "      <td>第四回 黑风双煞完颜洪熙笑道：“好，再打他个痛快。”蒙古兵前哨报来：“王罕亲自前来迎接大金国...</td>\n",
       "      <td>[第四回,  , 黑风双, 煞, 完颜洪熙, 笑, 道, ：, “, 好, ，, 再, 打,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5.0</td>\n",
       "      <td>第五回 弯弓射雕一行人下得山来，走不多时，忽听前面猛兽大吼声一阵阵传来。韩宝驹一提缰，胯下黄...</td>\n",
       "      <td>[第五回,  , 弯弓, 射雕, 一行, 人下, 得, 山来, ，, 走, 不多时, ，, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    txt  \\\n",
       "chap                                                      \n",
       "1.0   第一回 风雪惊变钱塘江浩浩江水，日日夜夜无穷无休的从两浙西路临安府牛家村边绕过，东流入海。江...   \n",
       "2.0   第二回 江南七怪颜烈跨出房门，过道中一个中年士人拖着鞋皮，踢跶踢跶的直响，一路打着哈欠迎面过...   \n",
       "3.0   第三回 黄沙莽莽寺里僧众见焦木圆寂，尽皆悲哭。有的便为伤者包扎伤处，抬入客舍。忽听得巨钟下的...   \n",
       "4.0   第四回 黑风双煞完颜洪熙笑道：“好，再打他个痛快。”蒙古兵前哨报来：“王罕亲自前来迎接大金国...   \n",
       "5.0   第五回 弯弓射雕一行人下得山来，走不多时，忽听前面猛兽大吼声一阵阵传来。韩宝驹一提缰，胯下黄...   \n",
       "\n",
       "                                                    cut  \n",
       "chap                                                     \n",
       "1.0   [第一回,  , 风雪, 惊变, 钱塘江, 浩浩, 江水, ，, 日日夜夜, 无穷, 无休,...  \n",
       "2.0   [第二回,  , 江南七怪, 颜烈, 跨出, 房门, ，, 过道, 中, 一个, 中年, 士...  \n",
       "3.0   [第三回,  , 黄沙, 莽莽, 寺里, 僧众, 见, 焦木, 圆寂, ，, 尽, 皆, 悲...  \n",
       "4.0   [第四回,  , 黑风双, 煞, 完颜洪熙, 笑, 道, ：, “, 好, ，, 再, 打,...  \n",
       "5.0   [第五回,  , 弯弓, 射雕, 一行, 人下, 得, 山来, ，, 走, 不多时, ，, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词和预处理，生成list of list格式\n",
    "import jieba\n",
    "\n",
    "chapter['cut'] = chapter.txt.apply(jieba.lcut)\n",
    "chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x4a8f4c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化word2vec模型和词表\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "n_dim = 300 # 指定向量维度，大样本量时300~500较好\n",
    "\n",
    "w2vmodel = Word2Vec(size = n_dim, min_count = 10)\n",
    "w2vmodel.build_vocab(chapter.cut) # 生成词表\n",
    "w2vmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***对word2vec模型进行训练***\n",
    "\n",
    ">word2vecmodel.train(\n",
    ">\n",
    "> sentences : iterable of iterables格式，对于特别大量的文本，尽量考虑流式处理\n",
    ">\n",
    ">total_examples = None : 句子总数，int，可直接使用model.corpus_count指定\n",
    ">\n",
    ">total_words = None : 句中词条总数，int，该参数和total_examples至少要指定一个\n",
    ">\n",
    ">epochs = None : 模型迭代次数，需要指定\n",
    ">\n",
    ">其他带默认值的参数设定：\n",
    ">\n",
    ">   start_alpha=None, end_alpha=None, word_count=0, queue_factor=2,\n",
    ">\n",
    ">   report_delay=1.0, compute_loss=False, callbacks=()\n",
    ">\n",
    ">)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3409733, 5975050)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在评论训练集上建模（大数据集时可能会花费几分钟）\n",
    "# 本例消耗内存较少\n",
    "#time \n",
    "w2vmodel.train(chapter.cut, \\\n",
    "               total_examples = w2vmodel.corpus_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.4949511 , -0.00842526, -0.2171623 ,  0.7543341 , -0.04970288,\n",
       "       -0.24352519,  0.53647155, -0.5041277 ,  0.8033062 ,  0.19798397,\n",
       "       -0.14323044,  0.04606513,  0.47363696,  0.01851266,  0.1761883 ,\n",
       "       -0.14015183, -0.19939011,  0.10235862,  0.03061349, -0.5380554 ,\n",
       "        0.49867526, -0.09631232,  0.6954292 , -0.10060424,  0.11374087,\n",
       "        0.63281137,  0.12074257, -0.23394747,  0.47420624,  0.47054785,\n",
       "        0.15025923, -0.9008722 , -0.05997993, -0.05178253, -0.5275885 ,\n",
       "       -0.0306479 ,  0.23682885, -0.01469101,  0.26895013, -0.23592488,\n",
       "        0.6329945 , -0.12705204, -0.48065042,  0.49208015,  0.49458185,\n",
       "        0.28157103, -0.2111737 ,  0.10984179, -0.23676407, -0.12948093,\n",
       "        0.17917465,  0.05942281,  0.39504817, -0.71047693,  0.44548586,\n",
       "        0.397497  , -0.01337112, -0.55380344, -1.0973364 , -0.13230963,\n",
       "       -0.6144392 , -0.14237542,  0.29108572,  0.35000265,  0.06871293,\n",
       "       -0.04935078,  0.23051688,  0.58580273, -0.0274988 ,  0.7953532 ,\n",
       "       -0.6373396 , -0.90750897, -0.1601246 ,  0.6683672 , -0.58423465,\n",
       "       -0.21351433,  0.08559965,  0.43084505, -0.8377945 ,  1.0198066 ,\n",
       "       -0.24073994, -1.0001819 ,  0.3412787 , -0.30413368, -0.23983745,\n",
       "        0.49908343,  0.32853547,  0.917403  ,  0.2832613 , -0.21663232,\n",
       "        0.59890825,  0.04949733, -0.18096104, -0.05199771,  0.02633479,\n",
       "       -0.17430657, -0.3005419 ,  0.6807373 , -0.03491025,  0.07652124,\n",
       "       -0.18377769, -0.29880333,  0.06223162, -0.05256068, -0.05380838,\n",
       "       -0.12447592,  0.00382502, -0.5782722 ,  0.1409262 , -0.40011406,\n",
       "        0.1026604 , -0.3514057 ,  0.27935016,  0.07517745, -0.06124368,\n",
       "       -0.38631994,  0.12879144, -0.18939331,  0.5824567 ,  0.05445927,\n",
       "        0.30462816, -0.0883755 , -0.16627648, -0.00444145, -0.40025857,\n",
       "        0.0653355 , -0.65780765,  0.02692085,  0.24809863, -0.5274541 ,\n",
       "       -0.1826128 ,  0.20470215, -0.34209615,  0.32011938,  0.26509547,\n",
       "        0.5173236 ,  0.55143255, -0.54323715, -0.02267838,  0.127046  ,\n",
       "        0.45435464, -0.7041053 ,  0.27058458, -0.7527051 ,  0.02260731,\n",
       "       -0.35448954,  0.29240644,  0.04537598,  0.2007952 ,  0.45704436,\n",
       "       -0.17368285,  0.47169903, -0.23576146,  0.8860329 , -0.43255642,\n",
       "       -0.17043155, -0.19770105,  0.37383687,  0.36022323,  0.05509621,\n",
       "       -0.07871534,  0.44481745,  0.86899334, -0.06230097,  0.32411742,\n",
       "       -0.8862718 ,  0.82966   , -0.12416975, -0.27857202,  0.04594105,\n",
       "        0.05747395,  0.23255517,  0.19066963, -0.17782967,  0.2824127 ,\n",
       "        1.3130366 , -0.5452303 , -0.0223141 , -0.10232034, -0.6582166 ,\n",
       "       -1.1522825 ,  0.35114735,  0.17699859,  0.32231173,  0.1134967 ,\n",
       "       -0.15988286, -0.6846314 ,  0.71272874,  0.4537169 ,  0.34169108,\n",
       "       -0.38144702, -0.35506168, -0.21439448,  0.0645122 , -0.2004804 ,\n",
       "        0.07616863,  0.12841715, -0.5208952 , -0.73560524, -0.40135056,\n",
       "       -0.214528  ,  0.33953193, -0.463889  ,  0.3710023 , -0.86526734,\n",
       "        0.31048012, -0.1116729 , -0.16824025,  0.12012487,  0.25511757,\n",
       "        0.5415559 ,  0.31998557, -0.6607693 ,  0.9025107 ,  0.23744375,\n",
       "        0.5951849 , -0.67187375, -0.12163592,  0.11974555,  0.34748083,\n",
       "        0.1552555 ,  0.60031766,  0.02285872,  0.12370332,  0.58870876,\n",
       "       -0.17887472, -0.32096025,  0.11871151, -0.29683903,  0.06263731,\n",
       "       -0.64679515, -0.17895943,  0.49294022, -0.10653292, -0.06512307,\n",
       "        0.375251  , -0.26817906, -0.2725664 , -0.22019018, -0.2853874 ,\n",
       "       -0.13154535,  0.20231712,  0.11313181,  1.3541068 , -0.34797063,\n",
       "        0.27892566, -0.01052187,  0.32807964, -0.17983636, -0.0685657 ,\n",
       "       -0.40569437,  0.12134277,  0.3544102 , -0.49556145, -0.48562357,\n",
       "       -0.4053053 , -0.06802417,  0.744951  ,  0.07545432, -0.45520616,\n",
       "        0.24888633,  0.1943477 ,  0.4411212 ,  0.13460091,  1.0922805 ,\n",
       "        0.53691334, -0.27872345,  0.05795967,  0.49935052, -0.4015359 ,\n",
       "        0.5466201 ,  0.0746438 ,  0.11740793,  0.35770407,  0.30777538,\n",
       "       -0.2750329 , -0.65943456,  0.64863294,  0.06815958,  0.46013886,\n",
       "       -0.65049005, -0.04838611, -0.6270964 , -0.242716  , -0.16001223,\n",
       "        1.0941578 , -1.0188501 ,  0.13588904, -0.62998796, -0.37637725,\n",
       "        0.19704983, -0.35749415,  0.47873974, -0.8464236 ,  0.3767519 ,\n",
       "       -0.31867948, -0.3167979 ,  0.12680303, -0.55732256,  0.3978725 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练完毕的模型实质\n",
    "print(w2vmodel.wv[\"郭靖\"].shape)\n",
    "w2vmodel.wv[\"郭靖\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v模型的保存和复用\n",
    "\n",
    "> w2vmodel.save(存盘路径及文件名称)\n",
    "> w2vmodel.load(存盘路径及文件名称)\n",
    "\n",
    "词向量间的相似度\n",
    "> w2vmodel.wv.most_similar(词条)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('黄蓉', 0.9139183163642883),\n",
       " ('欧阳克', 0.8371074199676514),\n",
       " ('穆念慈', 0.7574751377105713),\n",
       " ('程瑶迦', 0.7457101345062256),\n",
       " ('完颜康', 0.7375081181526184),\n",
       " ('柯镇恶', 0.7307557463645935),\n",
       " ('裘千仞', 0.7294843196868896),\n",
       " ('梅超风', 0.7269957065582275),\n",
       " ('杨康', 0.7257514595985413),\n",
       " ('穆易', 0.7191975116729736)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar(\"郭靖\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('郭靖', 0.9139183759689331),\n",
       " ('欧阳克', 0.8899412155151367),\n",
       " ('穆念慈', 0.8307197690010071),\n",
       " ('完颜康', 0.8280520439147949),\n",
       " ('程瑶迦', 0.8240326642990112),\n",
       " ('包惜弱', 0.8121179342269897),\n",
       " ('穆易', 0.7835402488708496),\n",
       " ('裘千仞', 0.7831634283065796),\n",
       " ('洪七公', 0.7824699282646179),\n",
       " ('陆冠英', 0.7788628339767456),\n",
       " ('周伯通', 0.7728621959686279),\n",
       " ('一灯', 0.7693881988525391),\n",
       " ('杨康', 0.7622434496879578),\n",
       " ('柯镇恶', 0.7571394443511963),\n",
       " ('那公子', 0.7453505992889404),\n",
       " ('欧阳锋', 0.741683840751648),\n",
       " ('华筝', 0.7287399768829346),\n",
       " ('鲁有脚', 0.7231466770172119),\n",
       " ('黄药师', 0.7229665517807007),\n",
       " ('梅超风', 0.7198961973190308)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar(\"黄蓉\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('郭靖道', 0.9571603536605835),\n",
       " ('傻姑道', 0.8834599852561951),\n",
       " ('朱聪道', 0.8736648559570312),\n",
       " ('杨康道', 0.8528420925140381),\n",
       " ('头道', 0.7910284996032715),\n",
       " ('怒道', 0.7819041013717651),\n",
       " ('郭靖摇', 0.7596696615219116),\n",
       " ('喜道', 0.748505711555481),\n",
       " ('那人道', 0.7314038276672363),\n",
       " ('黄蓉笑', 0.7236558198928833)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar(\"黄蓉道\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('奔', 0.8441349267959595),\n",
       " ('悄悄', 0.8150516152381897),\n",
       " ('过去', 0.8066216707229614),\n",
       " ('晕', 0.8034513592720032),\n",
       " ('推开', 0.8014492988586426)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 寻找对应关系\n",
    "w2vmodel.wv.most_similar(['郭靖', '小红马'], ['黄药师'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('欧阳克', 0.7425419092178345),\n",
       " ('洪七公', 0.6639356017112732),\n",
       " ('她', 0.6480711698532104),\n",
       " ('欧阳锋', 0.6412407755851746),\n",
       " ('梅超风', 0.6144818663597107),\n",
       " ('主人', 0.586811900138855),\n",
       " ('口中', 0.5859293937683105),\n",
       " ('周伯通', 0.5845122337341309),\n",
       " ('那公子', 0.5836194753646851),\n",
       " ('他', 0.5728038549423218)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar(positive=['郭靖', '黄蓉'], negative=['杨康'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9139184\n",
      "0.72575146\n",
      "0.66478753\n"
     ]
    }
   ],
   "source": [
    "# 计算两个词的相似度/相关程度\n",
    "print(w2vmodel.wv.similarity(\"郭靖\", \"黄蓉\"))\n",
    "print(w2vmodel.wv.similarity(\"郭靖\", \"杨康\"))\n",
    "print(w2vmodel.wv.similarity(\"郭靖\", \"杨铁心\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'小红马'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 寻找不合群的词\n",
    "w2vmodel.wv.doesnt_match(\"小红马 黄药师 鲁有脚\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'杨铁心'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.doesnt_match(\"杨铁心 黄药师 黄蓉 洪七公\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档相似度\n",
    "### 基于词袋模型计算\n",
    "***sklearn实现***\n",
    ">sklearn.metrics.pairwise.pairwise_distances(\n",
    ">\n",
    ">X : 用于计算距离的数组\n",
    ">\n",
    ">  \\[n_samples_a, n_samples_a\\] if metric == 'precomputed'\n",
    ">\n",
    ">   \\[n_samples_a, n_features\\] otherwise\n",
    ">\n",
    ">Y = None : 用于计算距离的第二数组，当metric != 'precomputed'时可用\n",
    ">\n",
    ">metric = 'euclidean' : 空间距离计算方式\n",
    ">\n",
    ">scikit-learn原生支持 : \\['cityblock', 'cosine', 'euclidean', \n",
    ">\n",
    ">        'l1', 'l2', 'manhattan'\\]，可直接使用稀疏矩阵格式\n",
    ">\n",
    ">    来自scipy.spatial.distance : \\['braycurtis', 'canberra', \n",
    ">\n",
    ">    'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard',\n",
    ">\n",
    ">        'kulsinski', 'mahalanobis', 'matching', 'minkowski',\n",
    ">\n",
    ">        'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener',\n",
    ">\n",
    ">        'sokalsneath', 'sqeuclidean', 'yule'\\] 不支持稀疏矩阵格式\n",
    ">\n",
    ">n_jobs = 1 : 用于计算的线程数，为-1时，所有CPU内核都用于计算\n",
    ">\n",
    ">)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm_cut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-acec42f02689>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcleanchap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcountvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-acec42f02689>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcleanchap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcountvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm_cut' is not defined"
     ]
    }
   ],
   "source": [
    "cleanchap = [ \" \".join(m_cut(w)) for w in chapter.txt.iloc[:5]] \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "resmtx = countvec.fit_transform(cleanchap)\n",
    "resmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resmtx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a6e19ae78ee6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresmtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cosine'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'resmtx' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "pairwise_distances(resmtx, metric = 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distances(resmtx) # 默认值为euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用TF-IDF矩阵进行相似度计算\n",
    "pairwise_distances(tfidf[:5], metric = 'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***gensim实现***\n",
    "\n",
    "***基于LDA计算余弦相似度***\n",
    "\n",
    "需要使用的信息：\n",
    "\n",
    "    拟合完毕的lda模型\n",
    "    按照拟合模型时矩阵种类转换的需检索文本\n",
    "        需检索的文本\n",
    "        建模时使用的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "simmtx = similarities.MatrixSimilarity(corpus)\n",
    "simmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索和第1章内容最相似（所属主题相同）的章节\n",
    "simmtx = similarities.MatrixSimilarity(corpus) # 使用的矩阵种类需要和拟合模型时相同\n",
    "simmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simmtx.index[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim的LDA拟合结果进行演示\n",
    "query = chapter.txt[1] \n",
    "query_bow = dictionary.doc2bow(m_cut(query))\n",
    "\n",
    "lda_vec = ldamodel[query_bow] # 转换为lda模型下的向量\n",
    "sims = simmtx[lda_vec] # 进行矩阵内向量和所提供向量的余弦相似度查询\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec\n",
    "word2vec用来计算词条相似度非常合适。\n",
    "\n",
    "较短的文档如果希望计算文本相似度，可以将各自内部的word2vec向量分别进行平均，用平均后的向量作为文本向量，从而用于计算相似度。\n",
    "\n",
    "但是对于长文档，这种平均的方式显然过于粗糙。\n",
    "\n",
    "doc2vec是word2vec的拓展，它可以直接获得sentences/paragraphs/documents的向量表达，从而可以进一步通过计算距离来得到sentences/paragraphs/documents之间的相似性。\n",
    "\n",
    "模型概况\n",
    "\n",
    "    分析目的：获得文档的一个固定长度的向量表达。\n",
    "    数据：多个文档，以及它们的标签，一般可以用标题作为标签。 \n",
    "    影响模型准确率的因素：语料的大小，文档的数量，越多越高；文档的相似性，越相似越好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "import gensim\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "def m_doc(doclist):\n",
    "    reslist = []\n",
    "    for i, doc in enumerate(doclist):\n",
    "        reslist.append(doc2vec.TaggedDocument(jieba.lcut(doc), [i]))\n",
    "    return reslist\n",
    "\n",
    "corp = m_doc(chapter.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel = gensim.models.Doc2Vec(vector_size = 300, \n",
    "                window = 20, min_count = 5)\n",
    "d2vmodel.build_vocab(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将新文本转换为相应维度空间下的向量\n",
    "newvec = d2vmodel.infer_vector(jieba.lcut(chapter.txt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel.docvecs.most_similar([newvec], topn = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
