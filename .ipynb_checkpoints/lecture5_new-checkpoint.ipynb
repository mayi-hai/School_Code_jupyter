{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键词提取\n",
    "\n",
    "### 用途\n",
    "\n",
    "* 用核心信息代表原始文档\n",
    "\n",
    "* 在文本聚类、分类、自动摘要等领域中有着重要应用\n",
    "\n",
    "### 需求：针对一篇文章，在不加人工干预的情况下提取关键词\n",
    "### 当然，首先要进行分词\n",
    "### 关键词分配： 事先给定关键词库，然后在文档中进行关键词检索\n",
    "### 关键词提取：根据某种规则，从文档中抽取最重要的词作为关键词\n",
    "* 有监督：抽取出候选词并标记是否为关键词，然后训练相应的模型\n",
    "\n",
    "* 无监督：给词条打分，并基于最高分值抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无监督方式的分析思路——基于词频\n",
    "\n",
    "### 分析思路1：按照词频高低进行提取\n",
    "\n",
    "* 大量的高频词并无多少意义（停用词）\n",
    "\n",
    "* 即使出现频率相同，常见词价值也明显低于不常见词\n",
    "\n",
    "### 分析思路2：按照词条在文档中的重要性进行提取\n",
    "\n",
    "* 如何确定词条在该文档中的重要性？\n",
    "\n",
    "常见的方法：**TF-IDF、网络图**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF算法\n",
    "\n",
    "### 信息检索（IR）中最常用的一种文本关键信息表示法\n",
    "\n",
    "### 基本思想：\n",
    "  \n",
    "* 如果某个词在一篇文档中出现频率较高，并且在语料库中其他文本中出现频率较低，甚至不出现，则认为这个词具有很好的类别区分能力\n",
    "\n",
    "### 词频TF：Term Frequency，衡量一个词在文档中出现的频率\n",
    "\n",
    "* 平均而言出现越频繁的词，其重要性可能就越高\n",
    "\n",
    "#### 考虑到文章长度的差异，需要对词频做标准化\n",
    "\n",
    "* TF(w) = (w出现在文档中的次数)/(文档中的词的总数)\n",
    "\n",
    "* TF(w) = (w出现在文档中的次数)/(文档中出现最多的词的次数)\n",
    "\n",
    "### 逆文档频率IDF：Inverse Document Frequency，用于模拟在该语料库中，某一个词有多重要\n",
    "\n",
    "* 有些词到处出现，但是明显是没有用的。比如各种停用词，过渡句用词等。\n",
    "\n",
    "* 因此把罕见的词的重要性（weight）调高，把常见词的重要性调低\n",
    "\n",
    "### IDF的具体算法\n",
    "\n",
    "* IDF(w) = log(语料库中的文档总数/(含有该w的文档总数+1))\n",
    "\n",
    "### TF-IDF = TF * IDF\n",
    "\n",
    "* TF-IDF与一个词在文档中的出现次数成正比\n",
    "\n",
    "* 与该词在整个语料中的出现次数成反比\n",
    "\n",
    "### 优点\n",
    "* 简单快速\n",
    "\n",
    "* 结果也比较符合实际情况\n",
    "\n",
    "### 缺点\n",
    "* 单纯以“词频”横量一个词的重要性，不够全面，有时重要的词可能出现的次数并不多\n",
    "\n",
    "* 无法考虑词与词之间的相互关系\n",
    "\n",
    "* 这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的\n",
    "\n",
    "  * 一种解决方式是，对全文的第一段和每一段的第一句话，给予较大的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF的具体实现\n",
    "\n",
    "jieba, NLTK, sklearn, gensim等程序包都可以实现TF-IDF的计算。除算法细节上会有差异外，更多的是数据输入/输出格式上的不同。\n",
    "\n",
    "### jieba\n",
    "\n",
    "输出结果会自动按照TF-IDF值降序排列，并且直接给出的是词条而不是字典ID，便于阅读使用。\n",
    "\n",
    "可在计算TF-IDF时直接完成分词，并使用停用词表和自定义词库，非常方便。\n",
    "\n",
    "有默认的IDF语料库，可以不训练模型，直接进行计算。\n",
    "\n",
    "以单个文本为单位进行分析。\n",
    "\n",
    "> jieba.analyse.extract_tags(\n",
    "> \n",
    "> sentence 为待提取的文本\n",
    "> \n",
    "> topK = 20 : 返回几个 TF/IDF 权重最大的关键词\n",
    ">\n",
    "> withWeight = False : 是否一并返回关键词权重值\n",
    ">\n",
    ">allowPOS = () : 仅包括指定词性的词，默认值为空，即不筛选\n",
    ">\n",
    ">)\n",
    "\n",
    "**jieba.analyse.set_idf_path(file_name)**\n",
    "\n",
    "    关键词提取时使用自定义逆向文件频率（IDF）语料库 \n",
    "\n",
    "> 劳动防护 13.900677652\n",
    ">\n",
    "> 生化学 13.900677652\n",
    ">\n",
    "> 奥萨贝尔 13.900677652\n",
    "> \n",
    "> 奧薩貝爾 13.900677652\n",
    "> \n",
    "> 考察队员 13.900677652\n",
    "\n",
    "**jieba.analyse.set_stop_words(file_name)**\n",
    "\n",
    "    关键词提取时使用自定义停止词（Stop Words）语料库 \n",
    "\n",
    "**jieba.analyse.TFIDF(idf_path = None)**\n",
    "\n",
    "    新建 TFIDF模型实例\n",
    "    idf_path : 读取已有的TFIDF频率文件（即已有模型）\n",
    "    使用该实例提取关键词：TFIDF实例.extract_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0230256f0eaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n\u001b[0;32m      4\u001b[0m                   names = ['txt'], sep ='aaa', encoding =\"GBK\" ,engine='python')\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrawgrp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mchapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrawgrp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m##.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mchapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchapter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[0;32m   7892\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7893\u001b[0m             \u001b[0mobserved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7894\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7895\u001b[0m         )\n\u001b[0;32m   7896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(obj, by, **kwds)\u001b[0m\n\u001b[0;32m   2520\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invalid type: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2522\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m                 \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                 \u001b[0mmutated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m             )\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'chap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 有的环境配置下read_table出错，因此改用read_csv\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"GBK\" ,engine='python')\n",
    "rawgrp = raw.groupby('chap')\n",
    "chapter = rawgrp.sum()##.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\n",
    "chapter = chapter[chapter.index != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chapter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2da044a54a7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 注意：函数是在使用默认的TFIDF模型进行分析！\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'chapter' is not defined"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "# 注意：函数是在使用默认的TFIDF模型进行分析！\n",
    "jieba.analyse.extract_tags(chapter.txt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.analyse.extract_tags(chapter.txt[1], withWeight = True) # 要求返回权重值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用自定义词典改善分词效果\n",
    "jieba.load_userdict('金庸小说词库.txt') # dict为自定义词典的路径\n",
    "\n",
    "# 在TFIDF计算中直接应用停用词表\n",
    "jieba.analyse.set_stop_words('停用词.txt')\n",
    "\n",
    "TFres = jieba.analyse.extract_tags(chapter.txt[1], withWeight = True)\n",
    "TFres[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自定义TF-IDF频率文件\n",
    "jieba.analyse.set_idf_path(\"idf.txt.big\")\n",
    "TFres1 = jieba.analyse.extract_tags(chapter.txt[1], withWeight = True)\n",
    "TFres1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "\n",
    "输出格式为矩阵，直接为后续的sklearn建模服务。\n",
    "\n",
    "需要先使用背景语料库进行模型训练。\n",
    "\n",
    "结果中给出的是字典ID而不是具体词条，直接阅读结果比较困难。\n",
    "\n",
    "class sklearn.feature_extraction.text.TfidfTransformer()\n",
    "\n",
    "发现参数基本上都不用动，所以这里就不介绍了.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "txtlist = [ \" \".join(m_cut(w)) for w in chapter.txt.iloc[:5]] \n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(txtlist) # 将文本中的词语转换为词频矩阵  \n",
    "\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(X)  #基于词频矩阵X计算TF-IDF值  \n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.toarray() # 转换为数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.todense() # 转换为矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"字典长度：\", len(vectorizer.vocabulary_))\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战练习\n",
    "\n",
    "请使用《射雕》全文计算出jieba分词的IDF语料库，然后使用该语料库重新对第一章计算关键词。比较这样的分析结果和以前有何不同。\n",
    "\n",
    "请自行编制将jieba分词的TF-IDF结果转换为文档-词条矩阵格式的程序。\n",
    "\n",
    "请自行思考本章提供的三种TF-IDF实现方式的使用场景是什么。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
