{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键词提取\n",
    "\n",
    "### 用途\n",
    "\n",
    "* 用核心信息代表原始文档\n",
    "\n",
    "* 在文本聚类、分类、自动摘要等领域中有着重要应用\n",
    "\n",
    "### 需求：针对一篇文章，在不加人工干预的情况下提取关键词\n",
    "### 当然，首先要进行分词\n",
    "### 关键词分配： 事先给定关键词库，然后在文档中进行关键词检索\n",
    "### 关键词提取：根据某种规则，从文档中抽取最重要的词作为关键词\n",
    "* 有监督：抽取出候选词并标记是否为关键词，然后训练相应的模型\n",
    "\n",
    "* 无监督：给词条打分，并基于最高分值抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无监督方式的分析思路——基于词频\n",
    "\n",
    "### 分析思路1：按照词频高低进行提取\n",
    "\n",
    "* 大量的高频词并无多少意义（停用词）\n",
    "\n",
    "* 即使出现频率相同，常见词价值也明显低于不常见词\n",
    "\n",
    "### 分析思路2：按照词条在文档中的重要性进行提取\n",
    "\n",
    "* 如何确定词条在该文档中的重要性？\n",
    "\n",
    "常见的方法：**TF-IDF、网络图**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF算法\n",
    "\n",
    "### 信息检索（IR）中最常用的一种文本关键信息表示法\n",
    "\n",
    "### 基本思想：\n",
    "  \n",
    "* 如果某个词在一篇文档中出现频率较高，并且在语料库中其他文本中出现频率较低，甚至不出现，则认为这个词具有很好的类别区分能力\n",
    "\n",
    "### 词频TF：Term Frequency，衡量一个词在文档中出现的频率\n",
    "\n",
    "* 平均而言出现越频繁的词，其重要性可能就越高\n",
    "\n",
    "#### 考虑到文章长度的差异，需要对词频做标准化\n",
    "\n",
    "* TF(w) = (w出现在文档中的次数)/(文档中的词的总数)\n",
    "\n",
    "* TF(w) = (w出现在文档中的次数)/(文档中出现最多的词的次数)\n",
    "\n",
    "### 逆文档频率IDF：Inverse Document Frequency，用于模拟在该语料库中，某一个词有多重要\n",
    "\n",
    "* 有些词到处出现，但是明显是没有用的。比如各种停用词，过渡句用词等。\n",
    "\n",
    "* 因此把罕见的词的重要性（weight）调高，把常见词的重要性调低\n",
    "\n",
    "### IDF的具体算法\n",
    "\n",
    "* IDF(w) = log(语料库中的文档总数/(含有该w的文档总数+1))\n",
    "\n",
    "### TF-IDF = TF * IDF\n",
    "\n",
    "* TF-IDF与一个词在文档中的出现次数成正比\n",
    "\n",
    "* 与该词在整个语料中的出现次数成反比\n",
    "\n",
    "### 优点\n",
    "* 简单快速\n",
    "\n",
    "* 结果也比较符合实际情况\n",
    "\n",
    "### 缺点\n",
    "* 单纯以“词频”横量一个词的重要性，不够全面，有时重要的词可能出现的次数并不多\n",
    "\n",
    "* 无法考虑词与词之间的相互关系\n",
    "\n",
    "* 这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的\n",
    "\n",
    "  * 一种解决方式是，对全文的第一段和每一段的第一句话，给予较大的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF的具体实现\n",
    "\n",
    "jieba, NLTK, sklearn, gensim等程序包都可以实现TF-IDF的计算。除算法细节上会有差异外，更多的是数据输入/输出格式上的不同。\n",
    "\n",
    "### jieba\n",
    "\n",
    "输出结果会自动按照TF-IDF值降序排列，并且直接给出的是词条而不是字典ID，便于阅读使用。\n",
    "\n",
    "可在计算TF-IDF时直接完成分词，并使用停用词表和自定义词库，非常方便。\n",
    "\n",
    "有默认的IDF语料库，可以不训练模型，直接进行计算。\n",
    "\n",
    "以单个文本为单位进行分析。\n",
    "\n",
    "> jieba.analyse.extract_tags(\n",
    "> \n",
    "> sentence 为待提取的文本\n",
    "> \n",
    "> topK = 20 : 返回几个 TF/IDF 权重最大的关键词\n",
    ">\n",
    "> withWeight = False : 是否一并返回关键词权重值\n",
    ">\n",
    ">allowPOS = () : 仅包括指定词性的词，默认值为空，即不筛选\n",
    ">\n",
    ">)\n",
    "\n",
    "**jieba.analyse.set_idf_path(file_name)**\n",
    "\n",
    "    关键词提取时使用自定义逆向文件频率（IDF）语料库 \n",
    "\n",
    "> 劳动防护 13.900677652\n",
    ">\n",
    "> 生化学 13.900677652\n",
    ">\n",
    "> 奥萨贝尔 13.900677652\n",
    "> \n",
    "> 奧薩貝爾 13.900677652\n",
    "> \n",
    "> 考察队员 13.900677652\n",
    "\n",
    "**jieba.analyse.set_stop_words(file_name)**\n",
    "\n",
    "    关键词提取时使用自定义停止词（Stop Words）语料库 \n",
    "\n",
    "**jieba.analyse.TFIDF(idf_path = None)**\n",
    "\n",
    "    新建 TFIDF模型实例\n",
    "    idf_path : 读取已有的TFIDF频率文件（即已有模型）\n",
    "    使用该实例提取关键词：TFIDF实例.extract_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 有的环境配置下read_table出错，因此改用read_csv\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"GBK\" ,engine='python')\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw['head'] = raw.txt.apply(m_head)\n",
    "raw['mid'] = raw.txt.apply(m_mid)\n",
    "raw['len'] = raw.txt.apply(len)\n",
    "# raw['chap'] = 0\n",
    "raw.head(0)\n",
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw)):\n",
    "    if raw['head'][i] == \"第\" and raw['mid'][i] > 0 and raw['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "del raw['len']\n",
    "raw.head(0)\n",
    "#提取章节\n",
    "rawgrp = raw.groupby('chap')\n",
    "chapter = rawgrp.sum()##.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\n",
    "chapter = chapter[chapter.index != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.879 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['杨铁心',\n",
       " '包惜弱',\n",
       " '郭啸天',\n",
       " '颜烈',\n",
       " '丘处机',\n",
       " '武官',\n",
       " '杨二人',\n",
       " '官兵',\n",
       " '曲三',\n",
       " '金兵',\n",
       " '那道人',\n",
       " '道长',\n",
       " '娘子',\n",
       " '段天德',\n",
       " '咱们',\n",
       " '临安',\n",
       " '说道',\n",
       " '丈夫',\n",
       " '杨家枪',\n",
       " '两人']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "# 注意：函数是在使用默认的TFIDF模型进行分析！\n",
    "jieba.analyse.extract_tags(chapter.txt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('杨铁心', 0.21886511509515091),\n",
       " ('包惜弱', 0.1685852913570757),\n",
       " ('郭啸天', 0.09908082913091291),\n",
       " ('颜烈', 0.05471627877378773),\n",
       " ('丘处机', 0.049556061537506184),\n",
       " ('武官', 0.04608486747703612),\n",
       " ('杨二人', 0.044305304110440376),\n",
       " ('官兵', 0.040144546232276104),\n",
       " ('曲三', 0.03439059290450272),\n",
       " ('金兵', 0.0336976598949901),\n",
       " ('那道人', 0.03117114380098961),\n",
       " ('道长', 0.02912588670625928),\n",
       " ('娘子', 0.026796070076125684),\n",
       " ('段天德', 0.025139911869037603),\n",
       " ('咱们', 0.023296768210644483),\n",
       " ('临安', 0.022991990912831523),\n",
       " ('说道', 0.022350916333591046),\n",
       " ('丈夫', 0.02221595763081643),\n",
       " ('杨家枪', 0.019765724469755074),\n",
       " ('两人', 0.0192267944114003)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.analyse.extract_tags(chapter.txt[1], withWeight = True) # 要求返回权重值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('杨铁心', 0.24766315655503918),\n",
       " ('包惜弱', 0.19076756653563828),\n",
       " ('郭啸天', 0.11211778033234882),\n",
       " ('曲三', 0.06421085701511758),\n",
       " ('颜烈', 0.061915789138759794),\n",
       " ('丘处机', 0.05607659595033594),\n",
       " ('武官', 0.05214866582927771),\n",
       " ('杨二人', 0.0501349493881299),\n",
       " ('官兵', 0.045426723368101905),\n",
       " ('金兵', 0.03813156251275196)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 应用自定义词典改善分词效果\n",
    "jieba.load_userdict('金庸小说词库.txt') # dict为自定义词典的路径\n",
    "\n",
    "# 在TFIDF计算中直接应用停用词表\n",
    "jieba.analyse.set_stop_words('停用词.txt')\n",
    "\n",
    "TFres = jieba.analyse.extract_tags(chapter.txt[1], withWeight = True)\n",
    "TFres[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('杨铁心', 0.24766315655503918),\n",
       " ('包惜弱', 0.19076756653563828),\n",
       " ('郭啸天', 0.11211778033234882),\n",
       " ('武官', 0.07028278767102464),\n",
       " ('颜烈', 0.061915789138759794),\n",
       " ('说道', 0.05856898972585386),\n",
       " ('丘处机', 0.05607659595033594),\n",
       " ('曲三', 0.05522219031294792),\n",
       " ('一个', 0.05354879060649496),\n",
       " ('杨二人', 0.05354879060649496)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义TF-IDF频率文件\n",
    "jieba.analyse.set_idf_path(\"idf.txt.big\")\n",
    "TFres1 = jieba.analyse.extract_tags(chapter.txt[1], withWeight = True)\n",
    "TFres1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "\n",
    "输出格式为矩阵，直接为后续的sklearn建模服务。\n",
    "\n",
    "需要先使用背景语料库进行模型训练。\n",
    "\n",
    "结果中给出的是字典ID而不是具体词条，直接阅读结果比较困难。\n",
    "\n",
    "class sklearn.feature_extraction.text.TfidfTransformer()\n",
    "\n",
    "发现参数基本上都不用动，所以这里就不介绍了.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm_cut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-404b981acf11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtxtlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-404b981acf11>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtxtlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm_cut' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "txtlist = [ \" \".join(m_cut(w)) for w in chapter.txt.iloc[:5]] \n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(txtlist) # 将文本中的词语转换为词频矩阵  \n",
    "\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(X)  #基于词频矩阵X计算TF-IDF值  \n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.toarray() # 转换为数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.todense() # 转换为矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"字典长度：\", len(vectorizer.vocabulary_))\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战练习\n",
    "\n",
    "请使用《射雕》全文计算出jieba分词的IDF语料库，然后使用该语料库重新对第一章计算关键词。比较这样的分析结果和以前有何不同。\n",
    "\n",
    "请自行编制将jieba分词的TF-IDF结果转换为文档-词条矩阵格式的程序。\n",
    "\n",
    "请自行思考本章提供的三种TF-IDF实现方式的使用场景是什么。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
